{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exciting-birthday",
   "metadata": {},
   "source": [
    "# Setting up a Schema in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-state",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-refund",
   "metadata": {},
   "source": [
    "In this lesson, we'll practice using Spark to change the datatypes of our schema.  We'll do so using the database of flood insurance claims.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pointed-lawrence",
   "metadata": {},
   "source": [
    "### Getting Set Up (For Google Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-planning",
   "metadata": {},
   "source": [
    "> If we are running this on google colab, we can run the following to eventually interact with our Spark UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-seller",
   "metadata": {},
   "source": [
    "* Begin by installing some pip packages and the java development kit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-waterproof",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark --quiet\n",
    "!pip install -U -q PyDrive --quiet \n",
    "!apt install openjdk-8-jdk-headless &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tight-south",
   "metadata": {},
   "source": [
    "* Then set the java environmental variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-violation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dangerous-congress",
   "metadata": {},
   "source": [
    "* Then connect to a SparkSession, setting the spark ui port to `4050`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-illinois",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"civComplaints\") \\\n",
    "    .config(\"spark.ui.port\", \"4050\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spread-recognition",
   "metadata": {},
   "source": [
    "* Then we need to install ngrok which will allow us to place our local spark ui on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ranging-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "get_ipython().system_raw('./ngrok http 4050 &')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plastic-bulletin",
   "metadata": {},
   "source": [
    "* And finally we get a link our Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-block",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "burning-moral",
   "metadata": {},
   "source": [
    "### Setting DataTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cathedral-physiology",
   "metadata": {},
   "source": [
    "Then let's load our data from the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "expected-lyric",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('s3://jigsaw-labs/houston_claims.csv', index_col = 0).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-poverty",
   "metadata": {},
   "source": [
    "And then load the data into a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "silent-tracy",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assisted-genre",
   "metadata": {},
   "source": [
    "Next use the `dtypes` method to see the format of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "indian-logging",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reportedCity', 'string'),\n",
       " ('dateOfLoss', 'string'),\n",
       " ('elevatedBuildingIndicator', 'string'),\n",
       " ('floodZone', 'string'),\n",
       " ('latitude', 'string'),\n",
       " ('longitude', 'string'),\n",
       " ('lowestFloodElevation', 'string'),\n",
       " ('amountPaidOnBuildingClaim', 'string'),\n",
       " ('amountPaidOnContentsClaim', 'string'),\n",
       " ('yearofLoss', 'string'),\n",
       " ('reportedZipcode', 'string'),\n",
       " ('id', 'string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims_df.dtypes\n",
    "\n",
    "# [('reportedCity', 'string'),\n",
    "#  ('dateOfLoss', 'string'),\n",
    "#  ('elevatedBuildingIndicator', 'string'),\n",
    "#  ('floodZone', 'string'),\n",
    "#  ('latitude', 'string'),\n",
    "#  ('longitude', 'string'),\n",
    "#  ('lowestFloodElevation', 'string'),\n",
    "#  ('amountPaidOnBuildingClaim', 'string'),\n",
    "#  ('amountPaidOnContentsClaim', 'string'),\n",
    "#  ('yearofLoss', 'string'),\n",
    "#  ('reportedZipcode', 'string'),\n",
    "#  ('id', 'string')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-homework",
   "metadata": {},
   "source": [
    "So we can see that multiple columns are not in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-exclusive",
   "metadata": {},
   "source": [
    "Begin by making the following changes. \n",
    "* Change `latitude` and `longitude` into floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "initial-tract",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, BooleanType, DateType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "square-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "exceptional-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_claims_df = claims_df \\\n",
    "    .withColumn(\"latitude\",col(\"latitude\").cast(FloatType())) \\\n",
    "    .withColumn(\"longitude\",col(\"longitude\").cast(FloatType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occupational-nurse",
   "metadata": {},
   "source": [
    "If we check the datatypes of `latitude` `longitude` and `id`, we should see the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "unexpected-flexibility",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[latitude: float, longitude: float]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_claims_df.select(['latitude', 'longitude'])\n",
    "# DataFrame[latitude: float, longitude: float, id: int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-adrian",
   "metadata": {},
   "source": [
    "And we should see the following values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fatal-aircraft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|latitude|longitude|\n",
      "+--------+---------+\n",
      "|    29.7|    -95.5|\n",
      "|    29.5|    -95.1|\n",
      "+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_claims_df.select(['latitude', 'longitude']).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-creek",
   "metadata": {},
   "source": [
    "Of course, we still have some additional columns that we should update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "spread-reduction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reportedCity: string (nullable = true)\n",
      " |-- dateOfLoss: string (nullable = true)\n",
      " |-- elevatedBuildingIndicator: string (nullable = true)\n",
      " |-- floodZone: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- lowestFloodElevation: string (nullable = true)\n",
      " |-- amountPaidOnBuildingClaim: string (nullable = true)\n",
      " |-- amountPaidOnContentsClaim: string (nullable = true)\n",
      " |-- yearofLoss: string (nullable = true)\n",
      " |-- reportedZipcode: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seeing-continent",
   "metadata": {},
   "source": [
    "So now let's make the following changes to the `amountPaid` and `yearOfLoss` columns:\n",
    "* Change the `amountPaidOnBuildingClaim` and columns `amountPaidOnContentsClaim` into floats, and rename the columns to `amount_paid_bldg`, and `amount_paid_contents` respectively.   \n",
    "* And change the `yearOfLoss` column into an integer, renaming the column to `year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "embedded-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_renamed_claims_df = updated_claims_df.withColumnRenamed('yearOfLoss', 'year') \\\n",
    ".withColumn(\"year\", col(\"year\").cast(IntegerType())) \\\n",
    ".withColumnRenamed(\"amountPaidOnBuildingClaim\", \"amount_paid_bldg\") \\\n",
    ".withColumn(\"amount_paid_bldg\",col(\"amount_paid_bldg\").cast(IntegerType())) \\\n",
    ".withColumnRenamed(\"amountPaidOnContentsClaim\", \"amount_paid_contents\") \\\n",
    ".withColumn(\"amount_paid_contents\",col(\"amount_paid_contents\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lonely-tamil",
   "metadata": {},
   "source": [
    "And then let's take a look at this new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "traditional-stable",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reportedCity', 'string'),\n",
       " ('dateOfLoss', 'string'),\n",
       " ('elevatedBuildingIndicator', 'string'),\n",
       " ('floodZone', 'string'),\n",
       " ('latitude', 'float'),\n",
       " ('longitude', 'float'),\n",
       " ('lowestFloodElevation', 'string'),\n",
       " ('amount_paid_bldg', 'int'),\n",
       " ('amount_paid_contents', 'int'),\n",
       " ('year', 'int'),\n",
       " ('reportedZipcode', 'string'),\n",
       " ('id', 'string')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_renamed_claims_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-imagination",
   "metadata": {},
   "source": [
    "Ok, so the remaining column to change is the `dateOfLoss` column.  Let's tackle this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-wallace",
   "metadata": {},
   "source": [
    "### Changing the date column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-austin",
   "metadata": {},
   "source": [
    "Finally change the `to_date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "victorian-requirement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          dateOfLoss|\n",
      "+--------------------+\n",
      "|2017-08-27T00:00:...|\n",
      "|2008-09-12T00:00:...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_renamed_claims_df.select('dateOfLoss').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "elementary-monte",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "southern-austin",
   "metadata": {},
   "outputs": [],
   "source": [
    "coerced_claims_df = updated_renamed_claims_df.withColumnRenamed('dateOfLoss', 'date'). \\\n",
    "withColumn(\"date\", col(\"date\").cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "after-interval",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               date|\n",
      "+-------------------+\n",
      "|2017-08-26 20:00:00|\n",
      "|2008-09-11 20:00:00|\n",
      "|2004-06-28 20:00:00|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coerced_claims_df.select('date').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-board",
   "metadata": {},
   "source": [
    "### Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "spectacular-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "coerced_claims_df.createOrReplaceTempView(\"claims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "baking-knitting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|year_of_claim|\n",
      "+-------------+\n",
      "|         2017|\n",
      "|         2008|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT year(date) as year_of_claim FROM claims LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "featured-appointment",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Pyspark Operations](https://hendra-herviawan.github.io/)\n",
    "\n",
    "[Spark SQL string Functions](https://sparkbyexamples.com/spark/usage-of-spark-sql-string-functions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-measurement",
   "metadata": {},
   "source": [
    "[Pyspark From Pandas](https://databricks.com/session/data-wrangling-with-pyspark-for-data-scientists-who-know-pandas)\n",
    "\n",
    "[Spark Tricks Blog](https://georgheiler.com/2019/02/22/dynamically-select-columns-by-type/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
