{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "stainless-surface",
   "metadata": {},
   "source": [
    "# Setting up a Schema in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bottom-pregnancy",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-stomach",
   "metadata": {},
   "source": [
    "Now in the last lesson, we were able to explore the dataset of civil complaints, but we did so with all of our data set to strings.  This will reduce our ability to explore, sort, and perform aggregations on our dataset.  In this lesson, we'll see how we can set the proper schema on a Spark dataframe.  \n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-nylon",
   "metadata": {},
   "source": [
    "### Getting Set Up (For Google Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-ready",
   "metadata": {},
   "source": [
    "> If we are running this on google colab, we can run the following to eventually interact with our Spark UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "varied-limitation",
   "metadata": {},
   "source": [
    "* Begin by installing some pip packages and the java development kit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "narrow-calgary",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark --quiet\n",
    "!pip install -U -q PyDrive --quiet \n",
    "!apt install openjdk-8-jdk-headless &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-brick",
   "metadata": {},
   "source": [
    "* Then set the java environmental variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-chase",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-doctrine",
   "metadata": {},
   "source": [
    "* Then connect to a SparkSession, setting the spark ui port to `4050`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-progress",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"civComplaints\") \\\n",
    "    .config(\"spark.ui.port\", \"4050\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "indoor-professional",
   "metadata": {},
   "source": [
    "* Then we need to install ngrok which will allow us to place our local spark ui on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "anonymous-mercury",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "get_ipython().system_raw('./ngrok http 4050 &')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collaborative-stuart",
   "metadata": {},
   "source": [
    "* And finally we get a link our Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-costa",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-czech",
   "metadata": {},
   "source": [
    "### Loading our Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-capture",
   "metadata": {},
   "source": [
    "Then, let's read in the our csv data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "european-kidney",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('s3://jigsaw-labs/civ_complaints.csv').astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "synthetic-moore",
   "metadata": {},
   "source": [
    "There are a lot of columns to this dataset, so let's set `vertical = True`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "strange-ivory",
   "metadata": {},
   "source": [
    "And then again, we'll create a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "rolled-trainer",
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-spotlight",
   "metadata": {},
   "source": [
    "And then let's take a look at the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "general-london",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Extract Run Date: string (nullable = true)\n",
      " |-- Randomized Id: string (nullable = true)\n",
      " |-- CCRB Received Year: string (nullable = true)\n",
      " |-- Days Between Incident Date and Received Date: string (nullable = true)\n",
      " |-- Case Type: string (nullable = true)\n",
      " |-- Complaint Received Place: string (nullable = true)\n",
      " |-- Complaint Received Mode: string (nullable = true)\n",
      " |-- Borough Of Incident: string (nullable = true)\n",
      " |-- Patrol Borough Of Incident: string (nullable = true)\n",
      " |-- Reason For Initial Contact: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complaints_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-arena",
   "metadata": {},
   "source": [
    "So we can see that all of the columns are currently set to datatype of string.  But if we set columns like `Extract Run Date` to datetimes, then we can perform calculations like selecting the month or day of week from those dates.  And even more columns should be numeric.  Ok, so let's see how we can set our columns to the correct datatypes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-invasion",
   "metadata": {},
   "source": [
    "### Exploring withColumn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "recent-forward",
   "metadata": {},
   "source": [
    "It turns out that we can use the `withColumn` method to change our data type.  Now the `withColumn` is generally used to derive new values from an existing column.  For example, if we look at the values in our `Randomized Id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "instant-accuracy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|Randomized Id|\n",
      "+-------------+\n",
      "|            1|\n",
      "|            2|\n",
      "|            3|\n",
      "+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complaints_df.select('Randomized Id').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continent-theory",
   "metadata": {},
   "source": [
    "And then we can used `withColumn` to increment each id by two like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "prospective-democrat",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_complaints_df = complaints_df.withColumn(\"id_plus_two\", \n",
    "                                                 col(\"Randomized Id\") + 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "southwest-friend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|Randomized Id|id_plus_two|\n",
      "+-------------+-----------+\n",
      "|            1|        3.0|\n",
      "|            2|        4.0|\n",
      "|            3|        5.0|\n",
      "|            4|        6.0|\n",
      "|            5|        7.0|\n",
      "+-------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_complaints_df.select([\"Randomized Id\", \"id_plus_two\"]).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceramic-growing",
   "metadata": {},
   "source": [
    "So we can see that the `withColumn` method created a new column called `id_plus_two`, and set the values as the column `col(\"Randomized Id\") + 2`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-fortune",
   "metadata": {},
   "source": [
    "### Setting Dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "published-shannon",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Extract Run Date', 'string'),\n",
       " ('Randomized Id', 'string'),\n",
       " ('CCRB Received Year', 'string'),\n",
       " ('Days Between Incident Date and Received Date', 'string'),\n",
       " ('Case Type', 'string'),\n",
       " ('Complaint Received Place', 'string'),\n",
       " ('Complaint Received Mode', 'string'),\n",
       " ('Borough Of Incident', 'string'),\n",
       " ('Patrol Borough Of Incident', 'string'),\n",
       " ('Reason For Initial Contact', 'string'),\n",
       " ('id', 'int')]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broadband-serial",
   "metadata": {},
   "source": [
    "Ok, so now let's use the `withColumn`, simply to change the values from `Randomized Id` from a string into an integer.  We can do so with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "initial-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "updated_df = complaints_df. \\\n",
    "            withColumn(\"id\", col(\"Randomized Id\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "mexican-indianapolis",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Extract Run Date', 'string'),\n",
       " ('Randomized Id', 'string'),\n",
       " ('CCRB Received Year', 'string'),\n",
       " ('Days Between Incident Date and Received Date', 'string'),\n",
       " ('Case Type', 'string'),\n",
       " ('Complaint Received Place', 'string'),\n",
       " ('Complaint Received Mode', 'string'),\n",
       " ('Borough Of Incident', 'string'),\n",
       " ('Patrol Borough Of Incident', 'string'),\n",
       " ('Reason For Initial Contact', 'string'),\n",
       " ('id', 'int')]"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hidden-swimming",
   "metadata": {},
   "source": [
    "So we can see that the above statement took the values of `Randomized Id` and cast them into an integer.\n",
    "\n",
    "Let's do this one more time changing the `Days Between Incident Date and Received Date` into a `Double`. \n",
    "\n",
    "> A double is a more precise version of a float (stores more decimal points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "acute-devon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "new_df = updated_df.withColumn(\"days_between\", col(\"Days Between Incident Date and Received Date\").cast(DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "harmful-niagara",
   "metadata": {},
   "source": [
    "And now if we look at the the datatypes, we can see that we now have a new column `days_between` of type `double`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "relevant-procurement",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Extract Run Date', 'string'),\n",
       " ('Randomized Id', 'string'),\n",
       " ('CCRB Received Year', 'string'),\n",
       " ('Days Between Incident Date and Received Date', 'string'),\n",
       " ('Case Type', 'string'),\n",
       " ('Complaint Received Place', 'string'),\n",
       " ('Complaint Received Mode', 'string'),\n",
       " ('Borough Of Incident', 'string'),\n",
       " ('Patrol Borough Of Incident', 'string'),\n",
       " ('Reason For Initial Contact', 'string'),\n",
       " ('id', 'int'),\n",
       " ('days_between', 'double')]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "seeing-supply",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|days_between|\n",
      "+------------+\n",
      "|         2.0|\n",
      "|        86.0|\n",
      "|         0.0|\n",
      "+------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_df.select(\"days_between\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disturbed-abraham",
   "metadata": {},
   "source": [
    "### Changing the Column Name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sapphire-citation",
   "metadata": {},
   "source": [
    "Now one thing we may notice is that when using the `withColumn` function we do successfully add a new, properly coerced column.  But the old column is still hanging around.  \n",
    "\n",
    "To correct that, there are a couple of things we can do.  One is to simply select the columns that are properly coerced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "dimensional-refrigerator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[id: int, days_between: double]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df.select(['id', 'days_between'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collect-camera",
   "metadata": {},
   "source": [
    "The other thing to do is to first change the column name, and then to coerce the data.  Let's see this. We'll start with our original dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "joined-footwear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Extract Run Date', 'string'),\n",
       " ('Randomized Id', 'string'),\n",
       " ('CCRB Received Year', 'string'),\n",
       " ('Days Between Incident Date and Received Date', 'string'),\n",
       " ('Case Type', 'string'),\n",
       " ('Complaint Received Place', 'string'),\n",
       " ('Complaint Received Mode', 'string'),\n",
       " ('Borough Of Incident', 'string'),\n",
       " ('Patrol Borough Of Incident', 'string'),\n",
       " ('Reason For Initial Contact', 'string')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amino-contract",
   "metadata": {},
   "source": [
    "And then we'll change the column name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "innovative-oakland",
   "metadata": {},
   "outputs": [],
   "source": [
    "renamed_df = complaints_df.withColumnRenamed(\"Randomized Id\", \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "lasting-college",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Extract Run Date', 'string'),\n",
       " ('id', 'string'),\n",
       " ('CCRB Received Year', 'string'),\n",
       " ('Days Between Incident Date and Received Date', 'string'),\n",
       " ('Case Type', 'string'),\n",
       " ('Complaint Received Place', 'string'),\n",
       " ('Complaint Received Mode', 'string'),\n",
       " ('Borough Of Incident', 'string'),\n",
       " ('Patrol Borough Of Incident', 'string'),\n",
       " ('Reason For Initial Contact', 'string')]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "renamed_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-deposit",
   "metadata": {},
   "source": [
    "So we can see that this successfully renamed the column, but did not change the datatype.  For that we'll have to go back to the `withColumn` method, but this time with both the new and old column name being the same -- here id.  \n",
    "\n",
    "Ok, here is both the `withColumnRenamed` method and the `withColumn` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "beneficial-oxide",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "updated_df = complaints_df.withColumnRenamed(\"Randomized Id\", \"id\"). \\\n",
    "withColumn(\"id\", col(\"id\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "combined-plaza",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Extract Run Date', 'string'),\n",
       " ('id', 'int'),\n",
       " ('CCRB Received Year', 'string'),\n",
       " ('Days Between Incident Date and Received Date', 'string'),\n",
       " ('Case Type', 'string'),\n",
       " ('Complaint Received Place', 'string'),\n",
       " ('Complaint Received Mode', 'string'),\n",
       " ('Borough Of Incident', 'string'),\n",
       " ('Patrol Borough Of Incident', 'string'),\n",
       " ('Reason For Initial Contact', 'string')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-wildlife",
   "metadata": {},
   "source": [
    "So we can see that this time, we first renamed the `Randomized Id` column to be called `id`, and then we updated that `id` column's values to be of type integer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "going-label",
   "metadata": {},
   "source": [
    "### Changing Date Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welsh-meeting",
   "metadata": {},
   "source": [
    "Ok, so by now, we may be getting a sense of how to change our column datatypes.  We can begin by changing the name of the column with the `withColumnRenamed` function, and from there we can use the `withColumn` function to cast the data to a new datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "handled-midwest",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "updated_df = complaints_df.withColumnRenamed(\"Randomized Id\", \"id\"). \\\n",
    "withColumn(\"id\", col(\"id\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unusual-bangkok",
   "metadata": {},
   "source": [
    "Ok, one datatype that is a little trickier to coerce to is the datetype.  For example, let's try changing the `Extract Run Date` column similarly to how we did above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "directed-poker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, IntegerType\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "funded-spank",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_complaints_df = complaints_df. \\\n",
    "    withColumnRenamed(\"Extract Run Date\", \"complaint_date\"). \\\n",
    "    withColumn(\"complaint_date\", col(\"complaint_date\").cast(DateType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "super-clearing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[complaint_date: date]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_complaints_df.select('complaint_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-optimum",
   "metadata": {},
   "source": [
    "But if we look at those values, we can see that they are all null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "herbal-corps",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|complaint_date|\n",
      "+--------------+\n",
      "|          null|\n",
      "|          null|\n",
      "+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_complaints_df.select('complaint_date').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-sugar",
   "metadata": {},
   "source": [
    "The issue is that our values did not start off in an easy to coerce format.  Let's take a look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "amino-stock",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|Extract Run Date|\n",
      "+----------------+\n",
      "|      05/25/2018|\n",
      "|      05/25/2018|\n",
      "+----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "complaints_df.select(\"Extract Run Date\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-sullivan",
   "metadata": {},
   "source": [
    "Ok, so to use the forward slashes, we'll need to do something like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "prostate-paint",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "updated_complaints_df = complaints_df. \\\n",
    "    withColumnRenamed(\"Extract Run Date\", \"complaint_date\"). \\\n",
    "    withColumn(\"complaint_date\", to_date(\"complaint_date\", \"MM/dd/yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "efficient-bosnia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|complaint_date|\n",
      "+--------------+\n",
      "|    2018-05-25|\n",
      "|    2018-05-25|\n",
      "+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_complaints_df.select(\"complaint_date\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-strategy",
   "metadata": {},
   "source": [
    "Ok, so now we have properly formatted our date column.  And the benefit of coercing our data, is now we can use our SQL methods to query our data.  Let's see this quickly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-rebound",
   "metadata": {},
   "source": [
    "### Briefly Using SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mediterranean-investigation",
   "metadata": {},
   "source": [
    "The only thing we need to do is first set the name of our dataframe to a table name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "assumed-investigator",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_complaints_df.createOrReplaceTempView(\"complaints\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opposed-background",
   "metadata": {},
   "source": [
    "And from there, we are ready to go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "respective-friendly",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|complaint_date|\n",
      "+--------------+\n",
      "|    2018-05-25|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT complaint_date from complaints LIMIT 1\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-outdoors",
   "metadata": {},
   "source": [
    "And can use the date functions seen [here](https://dwgeek.com/spark-sql-date-and-timestamp-functions-and-examples.html/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "revised-authority",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|year_of_complaint|\n",
      "+-----------------+\n",
      "|             2018|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT year(complaint_date)\n",
    "    as year_of_complaint FROM complaints LIMIT 1\"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-plaza",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guilty-sailing",
   "metadata": {},
   "source": [
    "In this lesson, we saw how to coerce our data in to the proper dataypes.  We did so by learning about the `withColumn` method, which creates a new column derived from an original column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fatal-reform",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Pyspark Operations](https://hendra-herviawan.github.io/)\n",
    "\n",
    "[Spark SQL string Functions](https://sparkbyexamples.com/spark/usage-of-spark-sql-string-functions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excited-wrist",
   "metadata": {},
   "source": [
    "[Pyspark From Pandas](https://databricks.com/session/data-wrangling-with-pyspark-for-data-scientists-who-know-pandas)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
