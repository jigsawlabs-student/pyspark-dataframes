{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "smooth-possibility",
   "metadata": {},
   "source": [
    "# Setting up a Schema in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-ensemble",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collected-limitation",
   "metadata": {},
   "source": [
    "In this lesson, we'll practice using Spark to change the datatypes of our schema.  We'll do so using the database of flood insurance claims.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adverse-liberia",
   "metadata": {},
   "source": [
    "### Getting Set Up (For Google Colab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unique-lucas",
   "metadata": {},
   "source": [
    "> If we are running this on google colab, we can run the following to eventually interact with our Spark UI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "designed-edgar",
   "metadata": {},
   "source": [
    "* Begin by installing some pip packages and the java development kit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blocked-hartford",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pyspark --quiet\n",
    "!pip install -U -q PyDrive --quiet \n",
    "!apt install openjdk-8-jdk-headless &> /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabulous-database",
   "metadata": {},
   "source": [
    "* Then set the java environmental variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continental-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "introductory-bundle",
   "metadata": {},
   "source": [
    "* Then connect to a SparkSession, setting the spark ui port to `4050`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opponent-connectivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"civComplaints\") \\\n",
    "    .config(\"spark.ui.port\", \"4050\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lined-dining",
   "metadata": {},
   "source": [
    "* Then we need to install ngrok which will allow us to place our local spark ui on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "innovative-plaza",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "!unzip ngrok-stable-linux-amd64.zip &> /dev/null\n",
    "get_ipython().system_raw('./ngrok http 4050 &')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numerous-private",
   "metadata": {},
   "source": [
    "* And finally we get a link our Spark UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "answering-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
    "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complicated-pipeline",
   "metadata": {},
   "source": [
    "### Setting DataTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-cleaning",
   "metadata": {},
   "source": [
    "Then let's load our data from the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dominant-security",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('s3://jigsaw-labs/houston_claims.csv', index_col = 0).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endless-netherlands",
   "metadata": {},
   "source": [
    "And then load the data into a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "qualified-kitchen",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continental-buying",
   "metadata": {},
   "source": [
    "Next use the `dtypes` method to see the format of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "crucial-treasure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reportedCity', 'string'),\n",
       " ('dateOfLoss', 'string'),\n",
       " ('elevatedBuildingIndicator', 'string'),\n",
       " ('floodZone', 'string'),\n",
       " ('latitude', 'string'),\n",
       " ('longitude', 'string'),\n",
       " ('lowestFloodElevation', 'string'),\n",
       " ('amountPaidOnBuildingClaim', 'string'),\n",
       " ('amountPaidOnContentsClaim', 'string'),\n",
       " ('yearofLoss', 'string'),\n",
       " ('reportedZipcode', 'string'),\n",
       " ('id', 'string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "# [('reportedCity', 'string'),\n",
    "#  ('dateOfLoss', 'string'),\n",
    "#  ('elevatedBuildingIndicator', 'string'),\n",
    "#  ('floodZone', 'string'),\n",
    "#  ('latitude', 'string'),\n",
    "#  ('longitude', 'string'),\n",
    "#  ('lowestFloodElevation', 'string'),\n",
    "#  ('amountPaidOnBuildingClaim', 'string'),\n",
    "#  ('amountPaidOnContentsClaim', 'string'),\n",
    "#  ('yearofLoss', 'string'),\n",
    "#  ('reportedZipcode', 'string'),\n",
    "#  ('id', 'string')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-campus",
   "metadata": {},
   "source": [
    "So we can see that multiple columns are not in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-campbell",
   "metadata": {},
   "source": [
    "Begin by making the following changes. \n",
    "* Change `latitude` and `longitude` into floats.\n",
    "> Use the [spark documentation](https://spark.apache.org/docs/latest/sql-ref-datatypes.html) if need be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "broadband-announcement",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-binary",
   "metadata": {},
   "source": [
    "If we check the datatypes of `latitude` `longitude` and `id`, we should see the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "extensive-council",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[latitude: float, longitude: float]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_claims_df.select(['latitude', 'longitude'])\n",
    "# DataFrame[latitude: float, longitude: float, id: int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "damaged-asthma",
   "metadata": {},
   "source": [
    "And we should see the following values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "infrared-fraction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|latitude|longitude|\n",
      "+--------+---------+\n",
      "|    29.7|    -95.5|\n",
      "|    29.5|    -95.1|\n",
      "+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_claims_df.select(['latitude', 'longitude']).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-fetish",
   "metadata": {},
   "source": [
    "Of course, we still have some additional columns that we should update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "divided-parker",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reportedCity: string (nullable = true)\n",
      " |-- dateOfLoss: string (nullable = true)\n",
      " |-- elevatedBuildingIndicator: string (nullable = true)\n",
      " |-- floodZone: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- lowestFloodElevation: string (nullable = true)\n",
      " |-- amountPaidOnBuildingClaim: string (nullable = true)\n",
      " |-- amountPaidOnContentsClaim: string (nullable = true)\n",
      " |-- yearofLoss: string (nullable = true)\n",
      " |-- reportedZipcode: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_claims_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "documentary-appliance",
   "metadata": {},
   "source": [
    "So now let's make the following changes to the `amountPaid` and `yearOfLoss` columns:\n",
    "* Change the `amountPaidOnBuildingClaim` and columns `amountPaidOnContentsClaim` into floats, and rename the columns to `amount_paid_bldg`, and `amount_paid_contents` respectively.   \n",
    "* And change the `yearOfLoss` column into an integer, renaming the column to `year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "processed-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_renamed_claims_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-requirement",
   "metadata": {},
   "source": [
    "And then let's take a look at this new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "reflected-mainstream",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reportedCity', 'string'),\n",
       " ('dateOfLoss', 'string'),\n",
       " ('elevatedBuildingIndicator', 'string'),\n",
       " ('floodZone', 'string'),\n",
       " ('latitude', 'float'),\n",
       " ('longitude', 'float'),\n",
       " ('lowestFloodElevation', 'string'),\n",
       " ('amount_paid_bldg', 'int'),\n",
       " ('amount_paid_contents', 'int'),\n",
       " ('year', 'int'),\n",
       " ('reportedZipcode', 'string'),\n",
       " ('id', 'string')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_renamed_claims_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-lemon",
   "metadata": {},
   "source": [
    "Ok, so the remaining column to change is the `dateOfLoss` column.  Let's tackle this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interracial-letters",
   "metadata": {},
   "source": [
    "### Changing the date column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-waterproof",
   "metadata": {},
   "source": [
    "Finally change the date of loss column.  If we look at it, we can see that it has a full time stamp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "massive-polish",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          dateOfLoss|\n",
      "+--------------------+\n",
      "|2017-08-27T00:00:...|\n",
      "|2008-09-12T00:00:...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_renamed_claims_df.select('dateOfLoss').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-yugoslavia",
   "metadata": {},
   "source": [
    "So we can change this column by renaming it from `dateOfLoss` to simply `date`, and changing the datatype to be of type `TimeStampType`.  \n",
    "\n",
    "> We do not need to use the `to_date` method here, as the `TimeStampType` can interpret this formatting out of the box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "broke-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fitted-church",
   "metadata": {},
   "outputs": [],
   "source": [
    "coerced_claims_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cheap-money",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               date|\n",
      "+-------------------+\n",
      "|2017-08-26 20:00:00|\n",
      "|2008-09-11 20:00:00|\n",
      "|2004-06-28 20:00:00|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coerced_claims_df.select('date').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "destroyed-nature",
   "metadata": {},
   "source": [
    "### Using SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prerequisite-domestic",
   "metadata": {},
   "source": [
    "Now that we have formatted our data we can begin to use SQL.  Do so by setting the name of the temporary view for our dataframe to `claims`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "established-screening",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "extended-belly",
   "metadata": {},
   "source": [
    "And then display the months that the first two claims were made, setting an alias of `month_of_claim`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "proof-tragedy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|year_of_claim|\n",
      "+-------------+\n",
      "|         2017|\n",
      "|         2008|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "helpful-musical",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Pyspark Operations](https://hendra-herviawan.github.io/)\n",
    "\n",
    "[Spark SQL string Functions](https://sparkbyexamples.com/spark/usage-of-spark-sql-string-functions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-plasma",
   "metadata": {},
   "source": [
    "[Pyspark From Pandas](https://databricks.com/session/data-wrangling-with-pyspark-for-data-scientists-who-know-pandas)\n",
    "\n",
    "[Spark Tricks Blog](https://georgheiler.com/2019/02/22/dynamically-select-columns-by-type/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
