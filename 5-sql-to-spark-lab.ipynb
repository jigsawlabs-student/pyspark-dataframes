{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "centered-mileage",
   "metadata": {},
   "source": [
    "# Setting up a Schema in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "front-reward",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-emperor",
   "metadata": {},
   "source": [
    "In this lesson, we'll practice using Spark to change the datatypes of our schema.  We'll do so using the database of flood insurance claims.  Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-tongue",
   "metadata": {},
   "source": [
    "### Getting Set Up"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-cabin",
   "metadata": {},
   "source": [
    "Let's get started by connecting to our Spark cluster with a spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alone-eligibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"houstonClaims\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "basic-circuit",
   "metadata": {},
   "source": [
    "Then, let's read in the our csv data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "academic-repeat",
   "metadata": {},
   "source": [
    "### Setting DataTypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "romantic-carbon",
   "metadata": {},
   "source": [
    "Then let's load our data from the csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "tutorial-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./houston_claims.csv', index_col = 0).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "purple-enhancement",
   "metadata": {},
   "source": [
    "And then load the data into a spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "optical-spectrum",
   "metadata": {},
   "outputs": [],
   "source": [
    "claims_df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-deadline",
   "metadata": {},
   "source": [
    "Next use the `dtypes` method to see the format of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "frank-idaho",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reportedCity', 'string'),\n",
       " ('dateOfLoss', 'string'),\n",
       " ('elevatedBuildingIndicator', 'string'),\n",
       " ('floodZone', 'string'),\n",
       " ('latitude', 'string'),\n",
       " ('longitude', 'string'),\n",
       " ('lowestFloodElevation', 'string'),\n",
       " ('amountPaidOnBuildingClaim', 'string'),\n",
       " ('amountPaidOnContentsClaim', 'string'),\n",
       " ('yearofLoss', 'string'),\n",
       " ('reportedZipcode', 'string'),\n",
       " ('id', 'string')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "claims_df.dtypes\n",
    "\n",
    "# [('reportedCity', 'string'),\n",
    "#  ('dateOfLoss', 'string'),\n",
    "#  ('elevatedBuildingIndicator', 'string'),\n",
    "#  ('floodZone', 'string'),\n",
    "#  ('latitude', 'string'),\n",
    "#  ('longitude', 'string'),\n",
    "#  ('lowestFloodElevation', 'string'),\n",
    "#  ('amountPaidOnBuildingClaim', 'string'),\n",
    "#  ('amountPaidOnContentsClaim', 'string'),\n",
    "#  ('yearofLoss', 'string'),\n",
    "#  ('reportedZipcode', 'string'),\n",
    "#  ('id', 'string')]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-right",
   "metadata": {},
   "source": [
    "So we can see that multiple columns are not in the correct format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-denver",
   "metadata": {},
   "source": [
    "Begin by making the following changes. \n",
    "* Change `latitude` and `longitude` into floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "improved-inspection",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DateType, BooleanType, DateType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "white-discipline",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "signed-damages",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_claims_df = claims_df \\\n",
    "    .withColumn(\"latitude\",col(\"latitude\").cast(FloatType())) \\\n",
    "    .withColumn(\"longitude\",col(\"longitude\").cast(FloatType()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-semiconductor",
   "metadata": {},
   "source": [
    "If we check the datatypes of `latitude` `longitude` and `id`, we should see the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "going-marble",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[latitude: float, longitude: float]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_claims_df.select(['latitude', 'longitude'])\n",
    "# DataFrame[latitude: float, longitude: float, id: int]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "failing-california",
   "metadata": {},
   "source": [
    "And we should see the following values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "strange-falls",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|latitude|longitude|\n",
      "+--------+---------+\n",
      "|    29.7|    -95.5|\n",
      "|    29.5|    -95.1|\n",
      "+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_claims_df.select(['latitude', 'longitude']).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charming-ability",
   "metadata": {},
   "source": [
    "Of course, we still have some additional columns that we should update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "demographic-tamil",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- reportedCity: string (nullable = true)\n",
      " |-- dateOfLoss: string (nullable = true)\n",
      " |-- elevatedBuildingIndicator: string (nullable = true)\n",
      " |-- floodZone: string (nullable = true)\n",
      " |-- latitude: float (nullable = true)\n",
      " |-- longitude: float (nullable = true)\n",
      " |-- lowestFloodElevation: string (nullable = true)\n",
      " |-- amountPaidOnBuildingClaim: string (nullable = true)\n",
      " |-- amountPaidOnContentsClaim: string (nullable = true)\n",
      " |-- yearofLoss: string (nullable = true)\n",
      " |-- reportedZipcode: string (nullable = true)\n",
      " |-- id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suffering-difference",
   "metadata": {},
   "source": [
    "So now let's make the following changes to the `amountPaid` and `yearOfLoss` columns:\n",
    "* Change the `amountPaidOnBuildingClaim` and columns `amountPaidOnContentsClaim` into floats, and rename the columns to `amount_paid_bldg`, and `amount_paid_contents` respectively.   \n",
    "* And change the `yearOfLoss` column into an integer, renaming the column to `year`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "written-cheat",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_renamed_claims_df = updated_claims_df.withColumnRenamed('yearOfLoss', 'year') \\\n",
    ".withColumn(\"year\", col(\"year\").cast(IntegerType())) \\\n",
    ".withColumnRenamed(\"amountPaidOnBuildingClaim\", \"amount_paid_bldg\") \\\n",
    ".withColumn(\"amount_paid_bldg\",col(\"amount_paid_bldg\").cast(IntegerType())) \\\n",
    ".withColumnRenamed(\"amountPaidOnContentsClaim\", \"amount_paid_contents\") \\\n",
    ".withColumn(\"amount_paid_contents\",col(\"amount_paid_contents\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-graduate",
   "metadata": {},
   "source": [
    "And then let's take a look at this new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "mathematical-advisory",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('reportedCity', 'string'),\n",
       " ('dateOfLoss', 'string'),\n",
       " ('elevatedBuildingIndicator', 'string'),\n",
       " ('floodZone', 'string'),\n",
       " ('latitude', 'float'),\n",
       " ('longitude', 'float'),\n",
       " ('lowestFloodElevation', 'string'),\n",
       " ('amount_paid_bldg', 'int'),\n",
       " ('amount_paid_contents', 'int'),\n",
       " ('year', 'int'),\n",
       " ('reportedZipcode', 'string'),\n",
       " ('id', 'string')]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "updated_renamed_claims_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "operating-petersburg",
   "metadata": {},
   "source": [
    "Ok, so the remaining column to change is the `dateOfLoss` column.  Let's tackle this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "english-aircraft",
   "metadata": {},
   "source": [
    "### Changing the date column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-kitty",
   "metadata": {},
   "source": [
    "Finally change the `to_date` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "rubber-opposition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          dateOfLoss|\n",
      "+--------------------+\n",
      "|2017-08-27T00:00:...|\n",
      "|2008-09-12T00:00:...|\n",
      "+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "updated_renamed_claims_df.select('dateOfLoss').show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "hazardous-bennett",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import TimestampType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "genetic-continuity",
   "metadata": {},
   "outputs": [],
   "source": [
    "coerced_claims_df = updated_renamed_claims_df.withColumnRenamed('dateOfLoss', 'date'). \\\n",
    "withColumn(\"date\", col(\"date\").cast(TimestampType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "owned-twelve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               date|\n",
      "+-------------------+\n",
      "|2017-08-26 20:00:00|\n",
      "|2008-09-11 20:00:00|\n",
      "|2004-06-28 20:00:00|\n",
      "+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coerced_claims_df.select('date').show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-singer",
   "metadata": {},
   "source": [
    "### Using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "incorrect-allocation",
   "metadata": {},
   "outputs": [],
   "source": [
    "coerced_claims_df.createOrReplaceTempView(\"claims\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "popular-portrait",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|year_of_claim|\n",
      "+-------------+\n",
      "|         2017|\n",
      "|         2008|\n",
      "+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT year(date) as year_of_claim FROM claims LIMIT 2\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "downtown-wellington",
   "metadata": {},
   "source": [
    "### Resources\n",
    "\n",
    "[Pyspark Operations](https://hendra-herviawan.github.io/)\n",
    "\n",
    "[Spark SQL string Functions](https://sparkbyexamples.com/spark/usage-of-spark-sql-string-functions/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compressed-baker",
   "metadata": {},
   "source": [
    "[Pyspark From Pandas](https://databricks.com/session/data-wrangling-with-pyspark-for-data-scientists-who-know-pandas)\n",
    "\n",
    "[Spark Tricks Blog](https://georgheiler.com/2019/02/22/dynamically-select-columns-by-type/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
